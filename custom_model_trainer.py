"""
è‡ªè¨‚æ¨¡å‹è¨“ç·´å™¨ - YOLOv8 å°ˆç”¨è»Šè¼›æª¢æ¸¬æ¨¡å‹è¨“ç·´
åŸºæ–¼ç”¨æˆ¶æ¨™è¨»è³‡æ–™è¨“ç·´å°ˆé–€çš„è»Šè¼›æª¢æ¸¬æ¨¡å‹
æ”¯æ´è³‡æ–™é›†åˆ†å‰²ã€è¨“ç·´ç›£æ§ã€    # ç¡¬é«”è¨­å®š
    device: str = "cpu"  # auto, cpu, cuda
    workers: int = 4ä¼°ç­‰å°ˆæ¥­åŠŸèƒ½
"""

import os
import sys
import yaml
import json
import shutil
import random
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass

import cv2
import numpy as np
from PIL import Image

try:
    from ultralytics import YOLO
    import torch
    from ultralytics.utils import LOGGER
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("è­¦å‘Šï¼šultralytics æœªå®‰è£ï¼Œæ¨¡å‹è¨“ç·´åŠŸèƒ½ä¸å¯ç”¨")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    print("è­¦å‘Šï¼šmatplotlib/seaborn æœªå®‰è£ï¼Œéƒ¨åˆ†è¦–è¦ºåŒ–åŠŸèƒ½ä¸å¯ç”¨")


@dataclass
class TrainingConfig:
    """è¨“ç·´é…ç½®åƒæ•¸"""
    # åŸºæœ¬è¨­å®š
    model_name: str = "custom_vehicle_model"
    base_model: str = "yolov8n.pt"  # åŸºç¤æ¨¡å‹
    
    # è³‡æ–™é›†è¨­å®š
    dataset_name: str = "vehicle_dataset"
    train_ratio: float = 0.7
    val_ratio: float = 0.2
    test_ratio: float = 0.1
    
    # è¨“ç·´åƒæ•¸
    epochs: int = 100
    batch_size: int = 16
    image_size: int = 640
    learning_rate: float = 0.01
    
    # å„ªåŒ–åƒæ•¸
    optimizer: str = "AdamW"
    weight_decay: float = 0.0005
    momentum: float = 0.937
    
    # æ•¸æ“šå¢å¼·
    augmentation: bool = True
    mixup: float = 0.1
    copy_paste: float = 0.3
    
    # ç¡¬é«”è¨­å®š
    device: str = "auto"  # auto, cpu, cuda
    workers: int = 8
    
    # é€²éšè¨­å®š
    patience: int = 50  # æ—©åœè€å¿ƒå€¼
    save_period: int = 10  # å„²å­˜é€±æœŸ
    resume: bool = False  # æ˜¯å¦æ¢å¾©è¨“ç·´
    
    def to_dict(self) -> Dict:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            'model_name': self.model_name,
            'base_model': self.base_model,
            'dataset_name': self.dataset_name,
            'train_ratio': self.train_ratio,
            'val_ratio': self.val_ratio,
            'test_ratio': self.test_ratio,
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'image_size': self.image_size,
            'learning_rate': self.learning_rate,
            'optimizer': self.optimizer,
            'weight_decay': self.weight_decay,
            'momentum': self.momentum,
            'augmentation': self.augmentation,
            'mixup': self.mixup,
            'copy_paste': self.copy_paste,
            'device': self.device,
            'workers': self.workers,
            'patience': self.patience,
            'save_period': self.save_period,
            'resume': self.resume
        }

    def get_optimal_device(self) -> str:
        """è‡ªå‹•æª¢æ¸¬æœ€ä½³è¨“ç·´è¨­å‚™"""
        if not ULTRALYTICS_AVAILABLE:
            return "cpu"
        
        try:
            import torch
            if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                device = f"cuda:{torch.cuda.current_device()}"
                print(f"ğŸš€ ä½¿ç”¨ GPU è¨“ç·´ï¼š{device}")
                return device
            else:
                print("ğŸ’» ä½¿ç”¨ CPU è¨“ç·´ï¼ˆæœªæª¢æ¸¬åˆ°å¯ç”¨çš„ GPUï¼‰")
                return "cpu"
        except Exception as e:
            print(f"âš ï¸ è¨­å‚™æª¢æ¸¬å¤±æ•—ï¼Œä½¿ç”¨ CPUï¼š{e}")
            return "cpu"


class DatasetPreparer:
    """è³‡æ–™é›†æº–å‚™å™¨"""
    
    def __init__(self, source_dir: str, output_dir: str):
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        
    def prepare_yolo_dataset(self, config: TrainingConfig, 
                           progress_callback: Optional[Callable] = None) -> str:
        """æº–å‚™YOLOæ ¼å¼çš„è¨“ç·´è³‡æ–™é›†"""
        
        print("ğŸ”„ æº–å‚™è¨“ç·´è³‡æ–™é›†...")
        
        # å‰µå»ºè³‡æ–™é›†ç›®éŒ„çµæ§‹
        dataset_path = self.output_dir / config.dataset_name
        self._create_dataset_structure(dataset_path)
        
        # æ”¶é›†åœ–ç‰‡å’Œæ¨™è¨»æª”æ¡ˆ
        image_files, label_files = self._collect_files()
        
        if not image_files:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•åœ–ç‰‡æª”æ¡ˆ")
        
        print(f"æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡")
        
        # åˆ†å‰²è³‡æ–™é›†
        train_files, val_files, test_files = self._split_dataset(
            image_files, config.train_ratio, config.val_ratio, config.test_ratio
        )
        
        print(f"è³‡æ–™é›†åˆ†å‰²: è¨“ç·´={len(train_files)}, é©—è­‰={len(val_files)}, æ¸¬è©¦={len(test_files)}")
        
        # è¤‡è£½æª”æ¡ˆåˆ°å°æ‡‰ç›®éŒ„
        total_files = len(train_files) + len(val_files) + len(test_files)
        processed = 0
        
        for split_name, file_list in [("train", train_files), ("val", val_files), ("test", test_files)]:
            for img_file in file_list:
                # è¤‡è£½åœ–ç‰‡
                img_dst = dataset_path / "images" / split_name / img_file.name
                shutil.copy2(img_file, img_dst)
                
                # è¤‡è£½æ¨™è¨»æª”æ¡ˆ
                label_file = self._find_label_file(img_file, label_files)
                if label_file:
                    label_dst = dataset_path / "labels" / split_name / f"{img_file.stem}.txt"
                    shutil.copy2(label_file, label_dst)
                
                processed += 1
                if progress_callback:
                    progress_callback(int(processed / total_files * 100))
        
        # å‰µå»ºè³‡æ–™é›†é…ç½®æª”æ¡ˆ
        self._create_dataset_yaml(dataset_path, config)
        
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
        self._generate_dataset_statistics(dataset_path)
        
        print(f"âœ… è³‡æ–™é›†æº–å‚™å®Œæˆï¼š{dataset_path}")
        return str(dataset_path / "dataset.yaml")
    
    def _create_dataset_structure(self, dataset_path: Path):
        """å‰µå»ºè³‡æ–™é›†ç›®éŒ„çµæ§‹"""
        for split in ["train", "val", "test"]:
            (dataset_path / "images" / split).mkdir(parents=True, exist_ok=True)
            (dataset_path / "labels" / split).mkdir(parents=True, exist_ok=True)
    
    def _collect_files(self) -> Tuple[List[Path], List[Path]]:
        """æ”¶é›†åœ–ç‰‡å’Œæ¨™è¨»æª”æ¡ˆ"""
        # æ”¯æ´çš„åœ–ç‰‡æ ¼å¼
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        
        image_files = []
        label_files = []
        
        # å¾å¤šå€‹å¯èƒ½çš„ä¾†æºæ”¶é›†æª”æ¡ˆ
        search_paths = [
            self.source_dir,
            self.source_dir / "images",
            Path("images"),
            Path("exports") / "yolo",
            Path("labels")
        ]
        
        for search_path in search_paths:
            if search_path.exists():
                # æ”¶é›†åœ–ç‰‡æª”æ¡ˆ
                for ext in image_extensions:
                    image_files.extend(search_path.glob(f"*{ext}"))
                
                # æ”¶é›†æ¨™è¨»æª”æ¡ˆ
                label_files.extend(search_path.glob("*.txt"))
        
        # å»é™¤é‡è¤‡
        image_files = list(set(image_files))
        label_files = list(set(label_files))
        
        return image_files, label_files
    
    def _find_label_file(self, image_file: Path, label_files: List[Path]) -> Optional[Path]:
        """ç‚ºåœ–ç‰‡æª”æ¡ˆæ‰¾åˆ°å°æ‡‰çš„æ¨™è¨»æª”æ¡ˆ"""
        image_stem = image_file.stem
        
        for label_file in label_files:
            if label_file.stem == image_stem:
                return label_file
        
        return None
    
    def _split_dataset(self, files: List[Path], train_ratio: float, 
                      val_ratio: float, test_ratio: float) -> Tuple[List[Path], List[Path], List[Path]]:
        """åˆ†å‰²è³‡æ–™é›†"""
        # æª¢æŸ¥æ¯”ä¾‹
        total_ratio = train_ratio + val_ratio + test_ratio
        if abs(total_ratio - 1.0) > 0.01:
            raise ValueError(f"è³‡æ–™é›†æ¯”ä¾‹ç¸½å’Œæ‡‰ç‚º1.0ï¼Œç•¶å‰ç‚º{total_ratio}")
        
        # éš¨æ©Ÿæ‰“äº‚
        files = files.copy()
        random.shuffle(files)
        
        total_files = len(files)
        train_count = int(total_files * train_ratio)
        val_count = int(total_files * val_ratio)
        
        train_files = files[:train_count]
        val_files = files[train_count:train_count + val_count]
        test_files = files[train_count + val_count:]
        
        return train_files, val_files, test_files
    
    def _create_dataset_yaml(self, dataset_path: Path, config: TrainingConfig):
        """å‰µå»ºè³‡æ–™é›†YAMLé…ç½®æª”æ¡ˆ"""
        
        # è¼‰å…¥è»Šç¨®é¡åˆ¥
        classes = self._load_vehicle_classes()
        
        yaml_content = {
            'path': str(dataset_path.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'test': 'images/test',
            'nc': len(classes),
            'names': {i: name for i, name in enumerate(classes)}
        }
        
        yaml_file = dataset_path / "dataset.yaml"
        with open(yaml_file, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)
        
        print(f"ğŸ“„ è³‡æ–™é›†é…ç½®æª”æ¡ˆï¼š{yaml_file}")
        print(f"ğŸ·ï¸ è»Šç¨®é¡åˆ¥ ({len(classes)}): {', '.join(classes)}")
    
    def _load_vehicle_classes(self) -> List[str]:
        """è¼‰å…¥è»Šç¨®é¡åˆ¥"""
        # å˜—è©¦å¾ vehicle_classes.json è¼‰å…¥
        try:
            from vehicle_class_manager import VehicleClassManager
            manager = VehicleClassManager()
            classes = manager.get_all_classes(enabled_only=True)
            return [cls.name for cls in classes]
        except:
            pass
        
        # å‚™ç”¨ï¼šå¾ classes.txt è¼‰å…¥
        classes_files = [
            Path("classes.txt"),
            Path("exports/yolo/classes.txt"),
            self.source_dir / "classes.txt"
        ]
        
        for classes_file in classes_files:
            if classes_file.exists():
                with open(classes_file, 'r', encoding='utf-8') as f:
                    return [line.strip() for line in f if line.strip()]
        
        # é è¨­é¡åˆ¥
        return ["æ©Ÿè»Š", "æ±½è»Š", "å¡è»Š", "å…¬è»Š"]
    
    def _generate_dataset_statistics(self, dataset_path: Path):
        """ç”Ÿæˆè³‡æ–™é›†çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'created_at': datetime.now().isoformat(),
            'dataset_path': str(dataset_path),
            'splits': {}
        }
        
        for split in ["train", "val", "test"]:
            images_dir = dataset_path / "images" / split
            labels_dir = dataset_path / "labels" / split
            
            image_count = len(list(images_dir.glob("*")))
            label_count = len(list(labels_dir.glob("*.txt")))
            
            # çµ±è¨ˆæ¨™è¨»æ•¸é‡
            total_annotations = 0
            class_counts = {}
            
            for label_file in labels_dir.glob("*.txt"):
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                    total_annotations += len(lines)
                    
                    for line in lines:
                        parts = line.strip().split()
                        if parts:
                            class_id = int(parts[0])
                            class_counts[class_id] = class_counts.get(class_id, 0) + 1
            
            stats['splits'][split] = {
                'images': image_count,
                'labels': label_count,
                'annotations': total_annotations,
                'class_distribution': class_counts
            }
        
        # å„²å­˜çµ±è¨ˆè³‡è¨Š
        stats_file = dataset_path / "dataset_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š è³‡æ–™é›†çµ±è¨ˆï¼š{stats_file}")


class ModelTrainer:
    """æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self, output_dir: str = "training_runs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.current_run_dir = None
        self.model = None
        self.training_config = None
        
    def train_model(self, dataset_yaml: str, config: TrainingConfig,
                   progress_callback: Optional[Callable] = None,
                   log_callback: Optional[Callable] = None) -> str:
        """è¨“ç·´è‡ªè¨‚æ¨¡å‹"""
        
        if not ULTRALYTICS_AVAILABLE:
            raise RuntimeError("ultralytics æœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œæ¨¡å‹è¨“ç·´")
        
        print("ğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´...")
        
        # å‰µå»ºè¨“ç·´åŸ·è¡Œç›®éŒ„
        self.current_run_dir = self._create_run_directory(config.model_name)
        
        # å„²å­˜è¨“ç·´é…ç½®
        self._save_training_config(config)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model_path = self._download_base_model(config.base_model)
        self.model = YOLO(model_path)
        
        # è¨­å®šè¨“ç·´åƒæ•¸
        train_args = self._prepare_training_args(dataset_yaml, config)
        
        # è¨­å®šå›èª¿å‡½æ•¸
        if progress_callback or log_callback:
            self._setup_callbacks(progress_callback, log_callback)
        
        try:
            # é–‹å§‹è¨“ç·´
            print(f"ğŸ“š è¼‰å…¥è³‡æ–™é›†ï¼š{dataset_yaml}")
            print(f"ğŸ¯ è¨“ç·´åƒæ•¸ï¼š{config.epochs} epochs, batch_size={config.batch_size}")
            
            results = self.model.train(**train_args)
            
            # å„²å­˜æœ€çµ‚æ¨¡å‹
            final_model_path = self._save_final_model(config.model_name)
            
            # ç”Ÿæˆè¨“ç·´å ±å‘Š
            self._generate_training_report(results, config)
            
            print(f"âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å„²å­˜æ–¼ï¼š{final_model_path}")
            return final_model_path
            
        except Exception as e:
            print(f"âŒ è¨“ç·´å¤±æ•—ï¼š{str(e)}")
            raise
    
    def evaluate_model(self, model_path: str, dataset_yaml: str) -> Dict:
        """è©•ä¼°æ¨¡å‹æ•ˆèƒ½"""
        
        print("ğŸ“Š è©•ä¼°æ¨¡å‹æ•ˆèƒ½...")
        
        if not ULTRALYTICS_AVAILABLE:
            raise RuntimeError("ultralytics æœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œæ¨¡å‹è©•ä¼°")
        
        # è¼‰å…¥æ¨¡å‹
        model = YOLO(model_path)
        
        # åœ¨é©—è­‰é›†ä¸Šè©•ä¼°
        results = model.val(data=dataset_yaml, split='val', save=True)
        
        # æå–è©•ä¼°æŒ‡æ¨™
        metrics = {
            'mAP50': float(results.box.map50),
            'mAP50-95': float(results.box.map),
            'precision': float(results.box.mp),
            'recall': float(results.box.mr),
            'f1_score': 2 * (float(results.box.mp) * float(results.box.mr)) / 
                       (float(results.box.mp) + float(results.box.mr) + 1e-16)
        }
        
        # æŒ‰é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
        if hasattr(results.box, 'ap_class_index') and hasattr(results.box, 'ap'):
            class_metrics = {}
            for i, class_idx in enumerate(results.box.ap_class_index):
                class_metrics[int(class_idx)] = {
                    'ap50': float(results.box.ap50[i]),
                    'ap': float(results.box.ap[i])
                }
            metrics['class_metrics'] = class_metrics
        
        print(f"ğŸ“ˆ è©•ä¼°çµæœï¼šmAP50={metrics['mAP50']:.3f}, mAP50-95={metrics['mAP50-95']:.3f}")
        
        return metrics
    
    def _create_run_directory(self, model_name: str) -> Path:
        """å‰µå»ºè¨“ç·´åŸ·è¡Œç›®éŒ„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_name = f"{model_name}_{timestamp}"
        run_dir = self.output_dir / run_name
        run_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ è¨“ç·´ç›®éŒ„ï¼š{run_dir}")
        return run_dir
    
    def _save_training_config(self, config: TrainingConfig):
        """å„²å­˜è¨“ç·´é…ç½®"""
        config_file = self.current_run_dir / "training_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config.to_dict(), f, indent=2, ensure_ascii=False)
    
    def _download_base_model(self, base_model: str) -> str:
        """ä¸‹è¼‰åŸºç¤æ¨¡å‹"""
        print(f"ğŸ“¥ æº–å‚™åŸºç¤æ¨¡å‹ï¼š{base_model}")
        
        # å¦‚æœæ˜¯æœ¬åœ°æª”æ¡ˆï¼Œç›´æ¥è¿”å›è·¯å¾‘
        if os.path.exists(base_model):
            return base_model
        
        # ä¸‹è¼‰é è¨“ç·´æ¨¡å‹
        try:
            model = YOLO(base_model)
            return base_model
        except Exception as e:
            print(f"ä¸‹è¼‰åŸºç¤æ¨¡å‹å¤±æ•—ï¼š{e}")
            # å‚™ç”¨æ¨¡å‹
            fallback_models = ["yolov8n.pt", "yolov8s.pt", "yolov8m.pt"]
            for fallback in fallback_models:
                try:
                    model = YOLO(fallback)
                    print(f"ä½¿ç”¨å‚™ç”¨æ¨¡å‹ï¼š{fallback}")
                    return fallback
                except:
                    continue
            
            raise RuntimeError("ç„¡æ³•ä¸‹è¼‰ä»»ä½•åŸºç¤æ¨¡å‹")
    
    def _prepare_training_args(self, dataset_yaml: str, config: TrainingConfig) -> Dict:
        """æº–å‚™è¨“ç·´åƒæ•¸"""
        # è‡ªå‹•æª¢æ¸¬æœ€ä½³è¨­å‚™
        optimal_device = config.get_optimal_device()
        
        args = {
            'data': dataset_yaml,
            'epochs': config.epochs,
            'batch': config.batch_size,
            'imgsz': config.image_size,
            'lr0': config.learning_rate,
            'optimizer': config.optimizer,
            'weight_decay': config.weight_decay,
            'momentum': config.momentum,
            'device': optimal_device,  # ä½¿ç”¨è‡ªå‹•æª¢æ¸¬çš„è¨­å‚™
            'workers': config.workers,
            'patience': config.patience,
            'save_period': config.save_period,
            'project': str(self.output_dir),
            'name': self.current_run_dir.name,
            'exist_ok': True,
            'save': True,
            'plots': True,
            'val': True
        }
        
        # æ•¸æ“šå¢å¼·è¨­å®š
        if config.augmentation:
            args.update({
                'mixup': config.mixup,
                'copy_paste': config.copy_paste,
                'degrees': 0.0,  # æ—‹è½‰è§’åº¦
                'translate': 0.1,  # å¹³ç§»
                'scale': 0.5,  # ç¸®æ”¾
                'shear': 0.0,  # å‰ªåˆ‡
                'perspective': 0.0,  # é€è¦–
                'flipud': 0.0,  # å‚ç›´ç¿»è½‰
                'fliplr': 0.5,  # æ°´å¹³ç¿»è½‰
                'mosaic': 1.0,  # é¦¬è³½å…‹å¢å¼·
                'hsv_h': 0.015,  # è‰²èª¿
                'hsv_s': 0.7,  # é£½å’Œåº¦
                'hsv_v': 0.4  # äº®åº¦
            })
        
        return args
    
    def _setup_callbacks(self, progress_callback: Optional[Callable], 
                        log_callback: Optional[Callable]):
        """è¨­å®šå›èª¿å‡½æ•¸"""
        
        def on_train_epoch_end(trainer):
            """è¨“ç·´epochçµæŸå›èª¿"""
            if progress_callback:
                progress = int((trainer.epoch + 1) / trainer.epochs * 100)
                progress_callback(progress)
            
            if log_callback:
                metrics = trainer.metrics if hasattr(trainer, 'metrics') else {}
                log_message = f"Epoch {trainer.epoch + 1}/{trainer.epochs}"
                if metrics:
                    # ç¢ºä¿ loss æ˜¯æ•¸å€¼é¡å‹
                    loss_value = metrics.get('loss', 'N/A')
                    if isinstance(loss_value, (int, float)):
                        log_message += f" - Loss: {loss_value:.4f}"
                    else:
                        log_message += f" - Loss: {loss_value}"
                log_callback(log_message)
        
        # è¨»å†Šå›èª¿
        if hasattr(self.model, 'add_callback'):
            self.model.add_callback('on_train_epoch_end', on_train_epoch_end)
    
    def _save_final_model(self, model_name: str) -> str:
        """å„²å­˜æœ€çµ‚æ¨¡å‹"""
        # æ‰¾åˆ°æœ€ä½³æ¬Šé‡
        weights_dir = self.current_run_dir / "weights"
        best_weights = weights_dir / "best.pt"
        last_weights = weights_dir / "last.pt"
        
        # é¸æ“‡æœ€ä½³æ¬Šé‡æª”æ¡ˆ
        if best_weights.exists():
            source_weights = best_weights
        elif last_weights.exists():
            source_weights = last_weights
        else:
            raise FileNotFoundError("æ‰¾ä¸åˆ°è¨“ç·´å¾Œçš„æ¬Šé‡æª”æ¡ˆ")
        
        # è¤‡è£½åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
        final_model_path = f"{model_name}.pt"
        shutil.copy2(source_weights, final_model_path)
        
        return final_model_path
    
    def _generate_training_report(self, results, config: TrainingConfig):
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        report = {
            'training_completed_at': datetime.now().isoformat(),
            'model_name': config.model_name,
            'base_model': config.base_model,
            'training_config': config.to_dict(),
            'final_metrics': {},
            'training_duration': None
        }
        
        # æå–æœ€çµ‚æŒ‡æ¨™
        if hasattr(results, 'metrics'):
            report['final_metrics'] = results.metrics
        
        # å„²å­˜å ±å‘Š
        report_file = self.current_run_dir / "training_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ è¨“ç·´å ±å‘Šï¼š{report_file}")


class TrainingVisualizer:
    """è¨“ç·´è¦–è¦ºåŒ–å·¥å…·"""
    
    def __init__(self, run_dir: str):
        self.run_dir = Path(run_dir)
        
    def plot_training_curves(self, save_path: Optional[str] = None):
        """ç¹ªè£½è¨“ç·´æ›²ç·š"""
        if not PLOTTING_AVAILABLE:
            print("matplotlib æœªå®‰è£ï¼Œç„¡æ³•ç”Ÿæˆè¨“ç·´æ›²ç·š")
            return
        
        # è®€å–è¨“ç·´çµæœ
        results_csv = self.run_dir / "results.csv"
        if not results_csv.exists():
            print("æ‰¾ä¸åˆ°è¨“ç·´çµæœæª”æ¡ˆ")
            return
        
        import pandas as pd
        
        # è®€å–è³‡æ–™
        df = pd.read_csv(results_csv)
        df.columns = df.columns.str.strip()  # æ¸…ç†æ¬„ä½åç¨±
        
        # å‰µå»ºåœ–è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('YOLOv8 Training Results', fontsize=16)
        
        # Loss æ›²ç·š
        axes[0, 0].plot(df['epoch'], df['train/box_loss'], label='Box Loss', linewidth=2)
        axes[0, 0].plot(df['epoch'], df['train/cls_loss'], label='Class Loss', linewidth=2)
        axes[0, 0].plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss', linewidth=2)
        axes[0, 0].set_title('Training Losses')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # mAP æ›²ç·š
        if 'metrics/mAP50(B)' in df.columns:
            axes[0, 1].plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP@0.5', linewidth=2, color='green')
            axes[0, 1].plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', linewidth=2, color='blue')
            axes[0, 1].set_title('Validation mAP')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('mAP')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # Precision & Recall
        if 'metrics/precision(B)' in df.columns:
            axes[1, 0].plot(df['epoch'], df['metrics/precision(B)'], label='Precision', linewidth=2, color='red')
            axes[1, 0].plot(df['epoch'], df['metrics/recall(B)'], label='Recall', linewidth=2, color='orange')
            axes[1, 0].set_title('Precision & Recall')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # Learning Rate
        if 'lr/pg0' in df.columns:
            axes[1, 1].plot(df['epoch'], df['lr/pg0'], label='Learning Rate', linewidth=2, color='purple')
            axes[1, 1].set_title('Learning Rate Schedule')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # å„²å­˜åœ–è¡¨
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è¨“ç·´æ›²ç·šå·²å„²å­˜ï¼š{save_path}")
        else:
            plt.savefig(self.run_dir / "training_curves.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è¨“ç·´æ›²ç·šï¼š{self.run_dir / 'training_curves.png'}")
        
        plt.close()
    
    def plot_confusion_matrix(self, class_names: List[str], save_path: Optional[str] = None):
        """ç¹ªè£½æ··æ·†çŸ©é™£"""
        if not PLOTTING_AVAILABLE:
            print("matplotlib æœªå®‰è£ï¼Œç„¡æ³•ç”Ÿæˆæ··æ·†çŸ©é™£")
            return
        
        # æŸ¥æ‰¾æ··æ·†çŸ©é™£æª”æ¡ˆ
        confusion_matrix_file = self.run_dir / "confusion_matrix_normalized.png"
        
        if confusion_matrix_file.exists():
            print(f"ğŸ“ˆ æ··æ·†çŸ©é™£ï¼š{confusion_matrix_file}")
        else:
            print("æ‰¾ä¸åˆ°æ··æ·†çŸ©é™£æª”æ¡ˆ")


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # æª¢æŸ¥ä¾è³´
    if not ULTRALYTICS_AVAILABLE:
        print("âŒ è«‹å…ˆå®‰è£ ultralyticsï¼špip install ultralytics")
        sys.exit(1)
    
    # è¨“ç·´é…ç½®
    config = TrainingConfig(
        model_name="custom_vehicle_detector",
        base_model="yolov8n.pt",
        epochs=50,
        batch_size=8,
        image_size=640
    )
    
    print("ğŸš— è‡ªè¨‚è»Šè¼›æª¢æ¸¬æ¨¡å‹è¨“ç·´")
    print("=" * 50)
    
    try:
        # æº–å‚™è³‡æ–™é›†
        preparer = DatasetPreparer(source_dir=".", output_dir="datasets")
        dataset_yaml = preparer.prepare_yolo_dataset(config)
        
        # è¨“ç·´æ¨¡å‹
        trainer = ModelTrainer(output_dir="training_runs")
        model_path = trainer.train_model(dataset_yaml, config)
        
        # è©•ä¼°æ¨¡å‹
        metrics = trainer.evaluate_model(model_path, dataset_yaml)
        print(f"ğŸ“Š æœ€çµ‚è©•ä¼°çµæœï¼š{metrics}")
        
        # ç”Ÿæˆè¦–è¦ºåŒ–
        visualizer = TrainingVisualizer(trainer.current_run_dir)
        visualizer.plot_training_curves()
        
        print("âœ… è‡ªè¨‚æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
